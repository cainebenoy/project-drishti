import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
import shap
import os
import joblib

class AnomalyDetector:
    def __init__(self, model_path='models'):
        self.model_path = model_path
        # Create models directory if it doesn't exist
        if not os.path.exists(self.model_path):
            os.makedirs(self.model_path)
            
    def train_and_score(self, df, contamination=0.01):
        """
        Trains Isolation Forest and calculates SHAP values.
        Args:
            df (pd.DataFrame): The master dataset.
            contamination (float): The expected proportion of outliers (default 1%).
        Returns:
            pd.DataFrame: The original dataframe with 'risk_status', 'anomaly_score', and 'primary_risk_factor'.
        """
        print("üß† [AI Engine] Initializing Neural Pathways...")
        
        # 1. Feature Selection
        # We use the 3 vectors we engineered in preprocessing
        features = ['adult_spike_ratio', 'velocity_index', 'ghost_ratio']
        
        # Sanity Check: Fill NaNs with 0 to prevent crashes
        X = df[features].fillna(0)
        
        # 2. Train Isolation Forest
        print(f"üß† [AI Engine] Training Isolation Forest on {len(X)} records...")
        # n_jobs=-1 uses all CPU cores for speed
        model = IsolationForest(contamination=contamination, random_state=42, n_jobs=-1)
        model.fit(X)
        
        # 3. Predict Anomalies
        # -1 = Anomaly, 1 = Normal
        df['anomaly_label'] = model.predict(X)
        # Decision function: Lower scores are more anomalous
        df['anomaly_score'] = model.decision_function(X)
        
        # 4. Explainability (SHAP)
        # This tells us WHY a specific row was flagged
        print("üß† [AI Engine] Calculating Explanations (SHAP)...")
        # TreeExplainer is optimized for tree-based models like Isolation Forest
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X)
        
        # We want to know which feature contributed MOST to the anomaly score
        shap_df = pd.DataFrame(shap_values, columns=features)
        
        # Find column with max absolute SHAP value for each row
        df['primary_risk_factor'] = shap_df.abs().idxmax(axis=1)
        
        # Convert numeric label to readable status
        df['risk_status'] = df['anomaly_label'].apply(lambda x: 'CRITICAL' if x == -1 else 'Low')
        
        # 5. Save Model for later use (optional, but good practice)
        joblib.dump(model, os.path.join(self.model_path, 'isolation_forest.pkl'))
        print(f"üíæ AI Model saved to {self.model_path}/isolation_forest.pkl")
        
        return df

if __name__ == "__main__":
    # Path to the data generated by preprocessing.py
    input_path = "data/master_processed_data.csv"
    output_path = "data/final_scored_data.csv"
    
    if os.path.exists(input_path):
        # Load Data
        df = pd.read_csv(input_path)
        
        # Run Engine
        detector = AnomalyDetector()
        scored_df = detector.train_and_score(df)
        
        # Save Results
        scored_df.to_csv(output_path, index=False)
        
        # Print Summary
        n_anomalies = len(scored_df[scored_df['risk_status'] == 'CRITICAL'])
        print(f"‚úÖ Analysis Complete.")
        print(f"   Total Pincodes Scanned: {len(df)}")
        print(f"   Anomalies Detected: {n_anomalies}")
        print(f"üíæ Final Intelligence Report saved to '{output_path}'")
        
    else:
        print("‚ùå Error: 'data/master_processed_data.csv' not found.")
        print("   Please run 'python src/preprocessing.py' first.")